{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5234bdd5-81ac-447a-832f-5e1820ce5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Run python -m visdom.server in the terminal before running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "322e8da6-e093-43d1-a115-f7f00a0c3c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import cv2\n",
    "import math\n",
    "from patchify import patchify\n",
    "from sklearn.model_selection import train_test_split\n",
    "from visdom import Visdom\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import model\n",
    "from Utils import *\n",
    "import spectral\n",
    "import sys\n",
    "import argparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e397aee-8340-422c-b44a-60df3d09b529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add --cuda flag manually for testing\n",
    "if not sys.argv:\n",
    "    sys.argv.append(\"\")\n",
    "\n",
    "if 'ipykernel_launcher' in sys.argv[0]:\n",
    "    sys.argv = [arg for arg in sys.argv if not arg.startswith(('-f', '/'))]\n",
    "\n",
    "sys.argv.append('--cuda')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--datasetname', default=\"PU\", help='IP,KSC,PU,SA,Houston')\n",
    "parser.add_argument('--numtrain', type=float, default=0.035, help='the number of train sets')\n",
    "parser.add_argument('--batchSize', type=int, default=32, help='batch size')\n",
    "parser.add_argument('--epochs', type=int, default=50, help='number of epochs to train for')\n",
    "parser.add_argument(\"--spectrumnum\", type=int, default=36, help=\"number of spectral after PCA\")\n",
    "parser.add_argument('--inputsize', type=int, default=11, help='size of input')\n",
    "parser.add_argument('--windowsize', type=int, default=3, help='size of windows')\n",
    "parser.add_argument(\n",
    "    \"--sampling_mode\",\n",
    "    type=str,\n",
    "    help=\"Sampling mode (random sampling or disjoint or fixed, default: random)\",\n",
    "    default=\"random\",\n",
    ")\n",
    "parser.add_argument(\"--input3D\", action=\"store_true\", default=False)\n",
    "parser.add_argument('--nz', type=int, default=50, help='size of the latent z vector')\n",
    "parser.add_argument('--D_lr', type=float, default=0.01, help='learning rate, default=0.001')\n",
    "parser.add_argument('--cuda', action='store_true', help='enables cuda')\n",
    "parser.add_argument('--netD', default='', help=\"path to netD (to continue training)\")\n",
    "parser.add_argument('--manualSeed', type=int, default=531, help='manual seed')\n",
    "parser.add_argument(\"--random_seed\", type=int, default=5, help=\"random seed\")  # Random seed\n",
    "opt = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5884962b-f36a-4ac5-8a05-a8c2fc835fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  531\n",
      "WARNING: You have a CUDA device, so you should probably run with --cuda\n"
     ]
    }
   ],
   "source": [
    "if opt.manualSeed is None:\n",
    "    opt.manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", opt.manualSeed)\n",
    "random.seed(opt.manualSeed)\n",
    "torch.manual_seed(opt.manualSeed)\n",
    "if opt.cuda:\n",
    "    torch.cuda.manual_seed_all(opt.manualSeed)\n",
    "cudnn.benchmark = False\n",
    "\n",
    "if torch.cuda.is_available() and not opt.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7ccfbbe-56fe-4154-9ab8-fbb25d7ca08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperspectral data shape: (610, 340, 103)\n",
      "Label shape: (610, 340)\n",
      "the number of sample: 42776\n",
      "Data shape after PCA : (610, 340, 36)\n",
      "after Xtrain shape: (1497, 36, 11, 11)\n",
      "after Xtest shape: (41279, 36, 11, 11)\n",
      "label 9\n"
     ]
    }
   ],
   "source": [
    "dataset = opt.datasetname\n",
    "X, y = loadData(dataset)\n",
    "H = X.shape[0]\n",
    "W = X.shape[1]\n",
    "pca_components = opt.spectrumnum\n",
    "print('Hyperspectral data shape:', X.shape)\n",
    "print('Label shape:', y.shape)\n",
    "sample_number = np.count_nonzero(y)\n",
    "print('the number of sample:', sample_number)\n",
    "X_pca = applyPCA(X, numComponents=pca_components)\n",
    "print('Data shape after PCA :', X_pca.shape)\n",
    "[nRow, nColumn, nBand] = X_pca.shape\n",
    "num_class = int(np.max(y))\n",
    "windowsize = opt.windowsize\n",
    "Wid = opt.inputsize\n",
    "halfsizeTL = int((Wid-1)/2)\n",
    "halfsizeBR = int((Wid-1)/2)\n",
    "paddedDatax = cv2.copyMakeBorder(X_pca, halfsizeTL, halfsizeBR, halfsizeTL, halfsizeBR, cv2.BORDER_CONSTANT, 0)  #cv2.BORDER_REPLICAT周围值\n",
    "paddedDatay = cv2.copyMakeBorder(y, halfsizeTL, halfsizeBR, halfsizeTL, halfsizeBR, cv2.BORDER_CONSTANT, 0)\n",
    "patchIndex = 0\n",
    "X_patch = np.zeros((sample_number, Wid, Wid, pca_components))\n",
    "y_patch = np.zeros(sample_number)\n",
    "for h in range(0, paddedDatax.shape[0]):\n",
    "    for w in range(0, paddedDatax.shape[1]):\n",
    "        if paddedDatay[h, w] == 0:\n",
    "            continue\n",
    "        X_patch[patchIndex, :, :, :] = paddedDatax[h-halfsizeTL:h+halfsizeBR+1, w-halfsizeTL:w+halfsizeBR+1, :]\n",
    "        X_patch[patchIndex] = paddedDatay[h, w]\n",
    "        patchIndex = patchIndex + 1\n",
    "X_train_p = patchify(paddedDatax, (Wid, Wid, pca_components), step=1)\n",
    "if opt.input3D:\n",
    "    X_train_p = X_train_p.reshape(-1, Wid, Wid, pca_components, 1)\n",
    "else:\n",
    "    X_train_p = X_train_p.reshape(-1, Wid, Wid, pca_components)\n",
    "y_train_p = y.reshape(-1)\n",
    "indices_0 = np.arange(y_train_p.size)\n",
    "X_train_q = X_train_p[y_train_p > 0, :, :, :]\n",
    "y_train_q = y_train_p[y_train_p > 0]\n",
    "indices_1 = indices_0[y_train_p > 0]\n",
    "y_train_q -= 1\n",
    "X_train_q = X_train_q.transpose(0, 3, 1, 2)\n",
    "Xtrain, Xtest, ytrain, ytest, idx1, idx2 = train_test_split(X_train_q, y_train_q, indices_1,\n",
    "                                                            train_size=opt.numtrain, random_state=opt.random_seed,\n",
    "                                                            stratify=y_train_q)\n",
    "print('after Xtrain shape:', Xtrain.shape)\n",
    "print('after Xtest shape:', Xtest.shape)\n",
    "\n",
    "trainset = TrainDS(Xtrain, ytrain)\n",
    "testset = TestDS(Xtest, ytest)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=opt.batchSize, shuffle=True, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testset, batch_size=opt.batchSize, shuffle=False, num_workers=0)\n",
    "nz = int(opt.nz)\n",
    "nc = pca_components\n",
    "nb_label = num_class\n",
    "print(\"label\", nb_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "619edac9-c819-4fd2-8e66-7101b21fe667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate total number of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Function to calculate OA, AA, and kappa\n",
    "def calculate_metrics(C):\n",
    "    OA = np.sum(np.diag(C)) / np.sum(C)\n",
    "    AA_ACC = np.diag(C) / np.sum(C, axis=1)\n",
    "    AA = np.mean(AA_ACC)\n",
    "    pe_row = np.sum(C, axis=0)\n",
    "    pe_col = np.sum(C, axis=1)\n",
    "    pe = np.dot(pe_row, pe_col) / np.sum(C)**2\n",
    "    kappa = (OA - pe) / (1 - pe)\n",
    "    return OA, AA, kappa, AA_ACC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fbcce86-3c65-495c-86e7-1030ba3e91c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvlab/anaconda3/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0\n",
      "LSGAVIT(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (conv3d_features): Sequential(\n",
      "      (0): Conv3d(1, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (1): BatchNorm3d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv2d_features): Sequential(\n",
      "      (0): Conv2d(144, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (proj): Conv2d(36, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (norm): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0): BasicLayer(\n",
      "      dim=120, input_resolution=(11, 11), depth=2\n",
      "      (blocks): ModuleList(\n",
      "        (0): LSGAVITBlock(\n",
      "          dim=120, input_resolution=(11, 11), num_heads=12, mlp_ratio=4.0\n",
      "          (norm1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): LSGAttention(\n",
      "            dim=120, num_heads=12\n",
      "            (qkv): Linear(in_features=120, out_features=120, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=120, out_features=120, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=120, out_features=480, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=480, out_features=120, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): LSGAVITBlock(\n",
      "          dim=120, input_resolution=(11, 11), num_heads=12, mlp_ratio=4.0\n",
      "          (norm1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): LSGAttention(\n",
      "            dim=120, num_heads=12\n",
      "            (qkv): Linear(in_features=120, out_features=120, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=120, out_features=120, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "          (norm2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=120, out_features=480, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=480, out_features=120, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (head): Linear(in_features=120, out_features=9, bias=True)\n",
      ")\n",
      "Total number of parameters: 545049\n",
      "[50/50][46/47]   D(x): -0.5610, errD_real: 0.0274,  Accuracy: 39509.0000 / 1497.0000 = 2639.2117\n",
      "\tTest set: Average loss: 0.1533, Accuracy: 39509/41279 (95.71%)\n",
      "OA= 95.71211 AA= 0.92589 k= 0.94318\n",
      "Classwise Accuracies: [0.97734021 0.99716604 0.79269497 0.86134596 1.         0.98660622\n",
      " 1.         0.84041655 0.87746171]\n",
      "Final OA: 95.71211\n",
      "Final AA: 0.92589\n",
      "Final Kappa: 0.94318\n",
      "Final classwise accuracies: [0.97734021 0.99716604 0.79269497 0.86134596 1.         0.98660622\n",
      " 1.         0.84041655 0.87746171]\n",
      "Training time: 312254.95 ms\n",
      "Testing time: 43609.12 ms\n"
     ]
    }
   ],
   "source": [
    "def train(netD, train_loader, test_loader):\n",
    "    viz = Visdom()\n",
    "    viz.close()\n",
    "\n",
    "    # Initialize accumulators for final results\n",
    "    final_acc = 0\n",
    "    final_aa = 0\n",
    "    final_kappa = 0\n",
    "    final_classwise_accuracies = None\n",
    "    \n",
    "    epoch_train_start = time.time()  # Start timing for training epoch\n",
    "    for epoch in range(1, opt.epochs + 1):\n",
    "        netD.train()\n",
    "        right = 0\n",
    "        for i, datas in enumerate(train_loader):\n",
    "            netD.zero_grad()\n",
    "            img, label = datas\n",
    "            batch_size = img.size(0)\n",
    "            input.resize_(img.size()).copy_(img)\n",
    "            c_label.resize_(batch_size).copy_(label)\n",
    "            c_output = netD(input)\n",
    "            c_errD_real = c_criterion(c_output, c_label)\n",
    "            errD_real = c_errD_real\n",
    "            errD_real.backward()\n",
    "            D_x = c_output.data.mean()\n",
    "            correct, length = test(c_output, c_label)\n",
    "            optimizerD.step()\n",
    "            right += correct\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            netD.eval()\n",
    "            test_loss = 0\n",
    "            right = 0\n",
    "            all_Label = []\n",
    "            all_target = []\n",
    "            epoch_test_start = time.time()  # Start timing for testing epoch\n",
    "            for data, target in test_loader:\n",
    "                indx_target = target.clone()\n",
    "                if opt.cuda:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                with torch.no_grad():\n",
    "                    data, target = Variable(data), Variable(target)\n",
    "\n",
    "                output = netD(data)\n",
    "                test_loss += c_criterion(output, target).item()\n",
    "                pred = output.max(1)[1]  # get the index of the max log-probability\n",
    "                all_Label.extend(pred)\n",
    "                all_target.extend(target)\n",
    "                right += pred.cpu().eq(indx_target).sum()\n",
    "\n",
    "            epoch_test_end = time.time()  # End timing for testing epoch\n",
    "\n",
    "            test_loss = test_loss / len(test_loader)  # average over number of mini-batch\n",
    "            acc = float(100. * float(right)) / float(len(test_loader.dataset))\n",
    "            AAA = torch.stack(all_target).data.cpu().numpy()\n",
    "            BBB = torch.stack(all_Label).data.cpu().numpy()\n",
    "            C = confusion_matrix(AAA, BBB)\n",
    "            C = C[:num_class, :num_class]\n",
    "            k = kappa(C, np.shape(C)[0])\n",
    "            AA_ACC = np.diag(C) / np.sum(C, 1)\n",
    "            AA = np.mean(AA_ACC, 0)\n",
    "\n",
    "            # Update final accumulators\n",
    "            final_acc = acc\n",
    "            final_aa = AA\n",
    "            final_kappa = k\n",
    "            final_classwise_accuracies = AA_ACC\n",
    "\n",
    "            # Print interim progress\n",
    "            print('[%d/%d][%d/%d]   D(x): %.4f, errD_real: %.4f,  Accuracy: %.4f / %.4f = %.4f'\n",
    "                  % (epoch, opt.epochs, i, len(train_loader),\n",
    "                     D_x, errD_real,\n",
    "                     right, len(train_loader.dataset), 100. * right / len(train_loader.dataset)))\n",
    "            print('\\tTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "                test_loss, right, len(test_loader.dataset), acc))\n",
    "            print('OA= %.5f AA= %.5f k= %.5f' % (acc, AA, k))\n",
    "            print('Classwise Accuracies:', AA_ACC)\n",
    "            \n",
    "    epoch_train_end = time.time()  # End timing for training epoch\n",
    "    \n",
    "    # Print final results\n",
    "    print('Final OA: {:.5f}'.format(final_acc))\n",
    "    print('Final AA: {:.5f}'.format(final_aa))\n",
    "    print('Final Kappa: {:.5f}'.format(final_kappa))\n",
    "    print('Final classwise accuracies:', final_classwise_accuracies)\n",
    "    print('Training time: {:.2f} ms'.format((epoch_train_end - epoch_train_start) * 1000))\n",
    "    print('Testing time: {:.2f} ms'.format((epoch_test_end - epoch_test_start) * 1000))\n",
    "\n",
    "for index_iter in range(1):\n",
    "    print('iter:', index_iter)\n",
    "    netD = model.LSGAVIT(img_size=Wid,\n",
    "                         patch_size=3,\n",
    "                         in_chans=pca_components,\n",
    "                         num_classes=num_class,\n",
    "                         embed_dim=120,\n",
    "                         depths=[2],\n",
    "                         num_heads=[12, 12, 12, 24],\n",
    "                         )\n",
    "    if opt.netD != '':\n",
    "        netD.load_state_dict(torch.load(opt.netD))\n",
    "    print(netD)\n",
    "    \n",
    "    total_params = count_parameters(netD)\n",
    "    print(f'Total number of parameters: {total_params}')\n",
    "    \n",
    "    c_criterion = nn.CrossEntropyLoss()\n",
    "    input = torch.FloatTensor(opt.batchSize, nc, opt.inputsize, opt.inputsize)\n",
    "    c_label = torch.LongTensor(opt.batchSize)\n",
    "    if opt.cuda:\n",
    "        netD.cuda()\n",
    "        c_criterion.cuda()\n",
    "        input = input.cuda()\n",
    "        c_label = c_label.cuda()\n",
    "    input = Variable(input)\n",
    "    c_label = Variable(c_label)\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=opt.D_lr)\n",
    "    train(netD, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d96599-26e2-47f3-bf1b-c18c149d3316",
   "metadata": {},
   "outputs": [],
   "source": [
    "### final training time = training time - testing time\n",
    "## calculate it after running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645bb75-f70c-40b6-88a5-4751c47628e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f0e898-90c8-4181-8059-c0e0d22e2313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
