{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "615fb01b-d7f5-478f-bb9f-0d411056796d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-12 15:36:03.283785: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-12 15:36:03.311273: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-12 15:36:03.311321: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-12 15:36:03.330604: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-12 15:36:04.310638: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spectral in /home/cvlab/anaconda3/lib/python3.11/site-packages (0.23.1)\n",
      "Requirement already satisfied: numpy in /home/cvlab/anaconda3/lib/python3.11/site-packages (from spectral) (1.23.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Conv2D, Conv3D, Flatten, Dense, Reshape, BatchNormalization\n",
    "from keras.layers import Dropout, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as Kb\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Activation\n",
    "from keras.layers import add, concatenate\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.utils import plot_model\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
    " \n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from operator import truediv\n",
    "\n",
    "\n",
    "from plotly.offline import init_notebook_mode\n",
    " \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import os\n",
    "!pip install spectral\n",
    "import spectral\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2daee46f-b8ad-4d65-873c-b1271d74b849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyFA(X, numComponents=75):\n",
    "    newX = np.reshape(X, (-1, X.shape[2]))\n",
    "    fa = FactorAnalysis(n_components=numComponents, random_state=0)\n",
    "    newX = fa.fit_transform(newX)\n",
    "    newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n",
    "    return newX, fa\n",
    "## GLOBAL VARIABLES\n",
    "dataset = 'SA'\n",
    "test_ratio = 0.95\n",
    "windowSize = 12\n",
    "def loadData(name):\n",
    "    data_path = os.path.join(os.getcwd(),'data')\n",
    "    if name == 'IP':\n",
    "        data = sio.loadmat(os.path.join(data_path, 'Indian_pines_corrected.mat'))['indian_pines_corrected']\n",
    "        labels = sio.loadmat(os.path.join(data_path, 'Indian_pines_gt.mat'))['indian_pines_gt']\n",
    "    elif name == 'SA':\n",
    "        data = sio.loadmat(os.path.join(data_path, 'Salinas_corrected.mat'))['salinas_corrected']\n",
    "        labels = sio.loadmat(os.path.join(data_path, 'Salinas_gt.mat'))['salinas_gt']\n",
    "    elif name == 'PU':\n",
    "        data = sio.loadmat(os.path.join(data_path, 'PaviaU.mat'))['paviaU']\n",
    "        labels = sio.loadmat(os.path.join(data_path, 'PaviaU_gt.mat'))['paviaU_gt']\n",
    "    \n",
    "    return data, labels\n",
    "def splitTrainTestSet(X, y, testRatio, randomState=345):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=randomState,\n",
    "                                                        stratify=y)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "def padWithZeros(X, margin=2):\n",
    "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
    "    x_offset = margin\n",
    "    y_offset = margin\n",
    "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
    "    return newX\n",
    "def createImageCubes(X, y, windowSize=8, removeZeroLabels = True):\n",
    "    margin = int((windowSize) / 2)\n",
    "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
    "    # split patches\n",
    "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n",
    "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n",
    "    patchIndex = 0\n",
    "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
    "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
    "            patch = zeroPaddedX[r - margin:r + margin , c - margin:c + margin ]   \n",
    "            patchesData[patchIndex, :, :, :] = patch\n",
    "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
    "            patchIndex = patchIndex + 1\n",
    "    if removeZeroLabels:\n",
    "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
    "        patchesLabels = patchesLabels[patchesLabels>0]\n",
    "        patchesLabels -= 1\n",
    "    return patchesData, patchesLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9376abba-2653-4854-9760-451ba3b14875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((512, 217, 204), (512, 217))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = loadData(dataset)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14b6f9a8-2046-4c3d-acf2-b296d103ab98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((54129, 12, 12, 204), (54129,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = createImageCubes(X, y, windowSize=windowSize)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1628d0e-4da5-41e9-99c6-85c3c91fd9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2706, 12, 12, 204), (51423, 12, 12, 204), (2706,), (51423,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = splitTrainTestSet(X, y, test_ratio)\n",
    "\n",
    "Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7be6228-b14b-4af0-9a5a-b42afa50f804",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = Xtrain.transpose(0, 3, 1, 2).reshape(2706, 1, 204, 12, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3f4a4ee-f1a8-4e03-a769-6b78a5f5d06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2706, 1, 204, 12, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2b96bc8-9e85-49f8-b6ab-d1407c65ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = Xtest.transpose(0, 3, 1, 2).reshape(51423, 1, 204, 12, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20e0c85e-9e77-4522-9a5b-76b391c79321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51423, 1, 204, 12, 12)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42580f39-ef7c-41d3-8b06-d701efaea90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([2706, 1, 204, 12, 12])\n",
      "320\n",
      "CTMixer(\n",
      "  650.93 k, 97.246% Params, 93.49 MMac, 99.666% MACs, \n",
      "  (act): ReLU(0, 0.000% Params, 36.86 KMac, 0.039% MACs, inplace=True)\n",
      "  (pad): ReplicationPad3d(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (0, 0, 0, 0, 0, 1))\n",
      "  (conv_1): Conv2d(13.44 k, 2.008% Params, 1.94 MMac, 2.063% MACs, 205, 320, kernel_size=(1, 1), stride=(1, 1), groups=5)\n",
      "  (bn_1): BatchNorm2d(640, 0.096% Params, 92.16 KMac, 0.098% MACs, 320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res0): Res(\n",
      "    555.94 k, 83.055% Params, 80.05 MMac, 85.340% MACs, \n",
      "    (conv1): Conv2d(184.64 k, 27.585% Params, 26.59 MMac, 28.343% MACs, 320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=5)\n",
      "    (bn1): BatchNorm2d(640, 0.096% Params, 92.16 KMac, 0.098% MACs, 320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(184.64 k, 27.585% Params, 26.59 MMac, 28.343% MACs, 320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=5)\n",
      "    (bn2): BatchNorm2d(640, 0.096% Params, 92.16 KMac, 0.098% MACs, 320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (res2): Res2(\n",
      "      185.38 k, 27.695% Params, 26.69 MMac, 28.456% MACs, \n",
      "      (conv1): Conv2d(92.19 k, 13.773% Params, 13.28 MMac, 14.152% MACs, 320, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, 0.010% Params, 9.22 KMac, 0.010% MACs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(92.48 k, 13.816% Params, 13.32 MMac, 14.196% MACs, 32, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): BatchNorm2d(640, 0.096% Params, 92.16 KMac, 0.098% MACs, 320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (bn1): BatchNorm2d(256, 0.038% Params, 36.86 KMac, 0.039% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv1): Conv2d(8.26 k, 1.233% Params, 1.19 MMac, 1.267% MACs, 128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (bn2): BatchNorm2d(128, 0.019% Params, 18.43 KMac, 0.020% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(640, 0.096% Params, 92.16 KMac, 0.098% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "  (bn3): BatchNorm2d(128, 0.019% Params, 18.43 KMac, 0.020% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(8.32 k, 1.243% Params, 1.2 MMac, 1.277% MACs, 64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (patch_embeddings): PatchEmbeddings(\n",
      "    41.09 k, 6.138% Params, 5.92 MMac, 6.307% MACs, \n",
      "    (patchify): Rearrange('b c (h p1) (w p2) -> b (h w) c p1 p2', p1=1, p2=1)\n",
      "    (flatten): Flatten(0, 0.000% Params, 0.0 Mac, 0.000% MACs, start_dim=2, end_dim=-1)\n",
      "    (proj): Linear(41.09 k, 6.138% Params, 5.92 MMac, 6.307% MACs, in_features=320, out_features=128, bias=True)\n",
      "  )\n",
      "  (pos_embeddings): PositionalEmbeddings(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "  (transformer): transformer(\n",
      "    19.78 k, 2.954% Params, 2.9 MMac, 3.095% MACs, \n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): Sequential(\n",
      "          2.05 k, 0.306% Params, 331.78 KMac, 0.354% MACs, \n",
      "          (0): LayerNorm(256, 0.038% Params, 18.43 KMac, 0.020% MACs, (128,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Attention_Conv(\n",
      "            1.79 k, 0.268% Params, 313.34 KMac, 0.334% MACs, \n",
      "            (attn): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)\n",
      "            (act): ReLU(0, 0.000% Params, 18.43 KMac, 0.020% MACs, inplace=True)\n",
      "            (bn): BatchNorm2d(256, 0.038% Params, 36.86 KMac, 0.039% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (qkv): Conv2d(768, 0.115% Params, 110.59 KMac, 0.118% MACs, 128, 768, kernel_size=(1, 1), stride=(1, 1), groups=128, bias=False)\n",
      "            (avgpool): AdaptiveAvgPool1d(0, 0.000% Params, 36.86 KMac, 0.039% MACs, output_size=128)\n",
      "            (qs): Conv2d(384, 0.057% Params, 55.3 KMac, 0.059% MACs, 128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), groups=128, bias=False)\n",
      "            (ks): Conv2d(384, 0.057% Params, 55.3 KMac, 0.059% MACs, 128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=128, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          17.73 k, 2.649% Params, 2.57 MMac, 2.741% MACs, \n",
      "          (0): LayerNorm(256, 0.038% Params, 18.43 KMac, 0.020% MACs, (128,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): FeedForward_Conv(\n",
      "            17.47 k, 2.610% Params, 2.55 MMac, 2.721% MACs, \n",
      "            (conv1): Sequential(\n",
      "              8.45 k, 1.262% Params, 1.23 MMac, 1.316% MACs, \n",
      "              (0): BatchNorm2d(256, 0.038% Params, 36.86 KMac, 0.039% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (1): GELU(0, 0.000% Params, 18.43 KMac, 0.020% MACs, approximate='none')\n",
      "              (2): Conv2d(8.19 k, 1.224% Params, 1.18 MMac, 1.258% MACs, 128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            )\n",
      "            (conv2): Sequential(\n",
      "              704, 0.105% Params, 110.59 KMac, 0.118% MACs, \n",
      "              (0): BatchNorm2d(128, 0.019% Params, 18.43 KMac, 0.020% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (1): GELU(0, 0.000% Params, 9.22 KMac, 0.010% MACs, approximate='none')\n",
      "              (2): Conv2d(576, 0.086% Params, 82.94 KMac, 0.088% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "            )\n",
      "            (conv3): Sequential(\n",
      "              8.32 k, 1.243% Params, 1.21 MMac, 1.287% MACs, \n",
      "              (0): BatchNorm2d(128, 0.019% Params, 18.43 KMac, 0.020% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (1): GELU(0, 0.000% Params, 9.22 KMac, 0.010% MACs, approximate='none')\n",
      "              (2): Conv2d(8.19 k, 1.224% Params, 1.18 MMac, 1.258% MACs, 64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.5, inplace=False)\n",
      "  (pool): Pooling(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "  (classifier): Classifier(\n",
      "    2.32 k, 0.347% Params, 2.19 KMac, 0.002% MACs, \n",
      "    (model): Sequential(\n",
      "      2.32 k, 0.347% Params, 2.19 KMac, 0.002% MACs, \n",
      "      (0): LayerNorm(256, 0.038% Params, 128.0 Mac, 0.000% MACs, (128,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(2.06 k, 0.308% Params, 2.06 KMac, 0.002% MACs, in_features=128, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "MACs: 93.81 MMac\n",
      "Parameters: 669.36 k\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from ptflops import get_model_complexity_info\n",
    "from transformer import transformer\n",
    "from embeddings import (PatchEmbeddings, CLSToken, PositionalEmbeddings)\n",
    "\n",
    "class Pooling(nn.Module):\n",
    "    def __init__(self, pool: str = \"mean\"):\n",
    "        super().__init__()\n",
    "        if pool not in [\"mean\", \"cls\"]:\n",
    "            raise ValueError(\"pool must be one of {mean, cls}\")\n",
    "\n",
    "        self.pool_fn = self.mean_pool if pool == \"mean\" else self.cls_pool\n",
    "\n",
    "    def mean_pool(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x.mean(dim=1)\n",
    "\n",
    "    def cls_pool(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x[:, 0]\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.pool_fn(x)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(in_features=dim, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "class Res2(nn.Module):\n",
    "    def __init__(self, in_channels, inter_channels, kernel_size, padding=0):\n",
    "        super(Res2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, inter_channels, kernel_size=kernel_size, padding=padding)\n",
    "        self.bn1 = nn.BatchNorm2d(inter_channels)\n",
    "        self.conv2 = nn.Conv2d(inter_channels, in_channels, kernel_size=kernel_size, padding=padding)\n",
    "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.bn1(self.conv1(X)))\n",
    "        X = self.bn2(self.conv2(X))\n",
    "        return X\n",
    "\n",
    "class Res(nn.Module):\n",
    "    def __init__(self, in_channels, kernel_size, padding, groups_s):\n",
    "        super(Res, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, padding=padding, groups=groups_s)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, padding=padding, groups=groups_s)\n",
    "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
    "        self.res2 = Res2(in_channels, 32, kernel_size=kernel_size, padding=padding)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        Z = self.res2(X)\n",
    "        return F.relu(X + Y + Z)\n",
    "\n",
    "class CTMixer(nn.Module):\n",
    "    def __init__(self, channels, num_classes, image_size, datasetname, num_layers: int=1, num_heads: int=4, \n",
    "                 patch_size: int = 1, emb_dim: int = 128, head_dim = 64, hidden_dim: int = 64, pool: str = \"mean\"):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.channels = channels\n",
    "        self.image_size = image_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.num_patch = int(math.sqrt(self.num_patches))\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        patch_dim = channels * patch_size ** 2\n",
    "\n",
    "        # Conv Preprocessing Module (Ref-SPRN)\n",
    "        if datasetname == 'IndianPines':\n",
    "            groups = 10  # Adjusted groups to match channels\n",
    "            groups_width = 40  # Adjusted width to match groups\n",
    "        elif datasetname == 'PaviaU':\n",
    "            groups = 5\n",
    "            groups_width = 64\n",
    "        elif datasetname == 'Salinas':\n",
    "            groups = 11\n",
    "            groups_width = 37\n",
    "        elif datasetname == 'Houston':\n",
    "            groups = 5\n",
    "            groups_width = 64\n",
    "        else:\n",
    "            groups = 11\n",
    "            groups_width = 37\n",
    "\n",
    "        new_bands = math.ceil(channels/groups) * groups\n",
    "        patch_dim = (groups*groups_width) * patch_size ** 2\n",
    "        pad_size = new_bands - channels\n",
    "        self.pad = nn.ReplicationPad3d((0, 0, 0, 0, 0, pad_size))\n",
    "        self.conv_1 = nn.Conv2d(new_bands, groups*groups_width, (1, 1), groups=groups)\n",
    "        self.bn_1 = nn.BatchNorm2d(groups*groups_width)\n",
    "        self.res0 = Res(groups*groups_width, (3, 3), (1, 1), groups_s=groups)\n",
    "\n",
    "        # Dual Residual Block (Ref-RDACN)\n",
    "        self.bn1 = nn.BatchNorm2d(emb_dim)\n",
    "        self.conv1 = nn.Conv2d(emb_dim, 64, kernel_size=1, padding=0)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1, groups=64)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, emb_dim, kernel_size=1, padding=0)\n",
    "\n",
    "        # Vision Transformer\n",
    "        self.patch_embeddings = PatchEmbeddings(patch_size=patch_size, patch_dim=patch_dim, emb_dim=emb_dim)\n",
    "        self.pos_embeddings = PositionalEmbeddings(num_pos=self.num_patches, dim=emb_dim)\n",
    "        self.transformer = transformer(dim=emb_dim, num_layers=num_layers, num_heads=num_heads, \n",
    "                                       head_dim=head_dim, hidden_dim=hidden_dim, num_patch=self.num_patch, patch_size=patch_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.pool = Pooling(pool=pool)\n",
    "        self.classifier = Classifier(dim=emb_dim, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pad(x).squeeze(1)\n",
    "        b, c, h, w = x.shape\n",
    "        x = F.relu(self.bn_1(self.conv_1(x)))\n",
    "        x = self.res0(x)\n",
    "\n",
    "        x4 = self.patch_embeddings(x)\n",
    "        x5 = self.pos_embeddings(x4)\n",
    "        x6 = self.transformer(x5)\n",
    "\n",
    "        x4_c = x4.reshape(b, -1, h, w)\n",
    "        x_c1 = self.conv1(self.act(self.bn1(x4_c)))\n",
    "        x_c2 = self.conv2(self.act(self.bn2(x_c1)))\n",
    "        x_c3 = self.conv3(self.act(self.bn3(x_c2)))\n",
    "\n",
    "        x7 = self.pool(self.dropout(x6 + x_c3.reshape(b, h*w, -1)))\n",
    "\n",
    "        return self.classifier(x7)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input = torch.randn(size=(2706, 1, 204, 12, 12))\n",
    "    print(\"input shape:\", input.shape)\n",
    "    model = CTMixer(channels=204, num_classes=16, image_size=12, datasetname='PaviaU', num_layers=1, num_heads=4)\n",
    "\n",
    "    # Get the model summary\n",
    "    summary(model, input_size=(2706, 1, 204, 12, 12), device=\"cpu\")\n",
    "\n",
    "    # Calculate the FLOPs\n",
    "    macs, params = get_model_complexity_info(model, (1, 204, 12, 12), as_strings=True, print_per_layer_stat=True)\n",
    "    print(f\"MACs: {macs}\")\n",
    "    print(f\"Parameters: {params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "486efe80-d8a7-4f03-9a4e-657ed4bd9e69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(Xtrain, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(ytrain, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(Xtest, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(ytest, dtype=torch.long)\n",
    "\n",
    "# Create custom datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc3d3da8-c0e1-41e6-bb33-08c6aa7e636f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CTMixer(\n",
       "  (act): ReLU(inplace=True)\n",
       "  (pad): ReplicationPad3d((0, 0, 0, 0, 0, 5))\n",
       "  (conv_1): Conv2d(209, 407, kernel_size=(1, 1), stride=(1, 1), groups=11)\n",
       "  (bn_1): BatchNorm2d(407, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (res0): Res(\n",
       "    (conv1): Conv2d(407, 407, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=11)\n",
       "    (bn1): BatchNorm2d(407, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(407, 407, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=11)\n",
       "    (bn2): BatchNorm2d(407, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (res2): Res2(\n",
       "      (conv1): Conv2d(407, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 407, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(407, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (patch_embeddings): PatchEmbeddings(\n",
       "    (patchify): Rearrange('b c (h p1) (w p2) -> b (h w) c p1 p2', p1=1, p2=1)\n",
       "    (flatten): Flatten(start_dim=2, end_dim=-1)\n",
       "    (proj): Linear(in_features=407, out_features=128, bias=True)\n",
       "  )\n",
       "  (pos_embeddings): PositionalEmbeddings()\n",
       "  (transformer): transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention_Conv(\n",
       "            (attn): Softmax(dim=-1)\n",
       "            (act): ReLU(inplace=True)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (qkv): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), groups=128, bias=False)\n",
       "            (avgpool): AdaptiveAvgPool1d(output_size=128)\n",
       "            (qs): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), groups=128, bias=False)\n",
       "            (ks): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=128, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForward_Conv(\n",
       "            (conv1): Sequential(\n",
       "              (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (conv2): Sequential(\n",
       "              (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "            )\n",
       "            (conv3): Sequential(\n",
       "              (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (pool): Pooling()\n",
       "  (classifier): Classifier(\n",
       "    (model): Sequential(\n",
       "      (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = CTMixer(channels=204, num_classes=16, image_size=12, datasetname='Salinas', num_layers=1, num_heads=4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4979640-2779-40b9-b54e-4f94a0bcfb22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.9738\n",
      "Epoch [2/50], Loss: 0.6351\n",
      "Epoch [3/50], Loss: 0.4097\n",
      "Epoch [4/50], Loss: 0.3627\n",
      "Epoch [5/50], Loss: 0.2661\n",
      "Epoch [6/50], Loss: 0.2641\n",
      "Epoch [7/50], Loss: 0.1852\n",
      "Epoch [8/50], Loss: 0.1791\n",
      "Epoch [9/50], Loss: 0.2223\n",
      "Epoch [10/50], Loss: 0.1294\n",
      "Epoch [11/50], Loss: 0.1408\n",
      "Epoch [12/50], Loss: 0.1170\n",
      "Epoch [13/50], Loss: 0.1611\n",
      "Epoch [14/50], Loss: 0.1103\n",
      "Epoch [15/50], Loss: 0.1024\n",
      "Epoch [16/50], Loss: 0.0892\n",
      "Epoch [17/50], Loss: 0.1604\n",
      "Epoch [18/50], Loss: 0.1036\n",
      "Epoch [19/50], Loss: 0.0645\n",
      "Epoch [20/50], Loss: 0.0532\n",
      "Epoch [21/50], Loss: 0.0869\n",
      "Epoch [22/50], Loss: 0.0945\n",
      "Epoch [23/50], Loss: 0.0934\n",
      "Epoch [24/50], Loss: 0.0909\n",
      "Epoch [25/50], Loss: 0.0783\n",
      "Epoch [26/50], Loss: 0.0679\n",
      "Epoch [27/50], Loss: 0.0659\n",
      "Epoch [28/50], Loss: 0.0576\n",
      "Epoch [29/50], Loss: 0.0578\n",
      "Epoch [30/50], Loss: 0.0417\n",
      "Epoch [31/50], Loss: 0.0818\n",
      "Epoch [32/50], Loss: 0.0464\n",
      "Epoch [33/50], Loss: 0.0351\n",
      "Epoch [34/50], Loss: 0.0532\n",
      "Epoch [35/50], Loss: 0.0353\n",
      "Epoch [36/50], Loss: 0.0413\n",
      "Epoch [37/50], Loss: 0.0355\n",
      "Epoch [38/50], Loss: 0.0379\n",
      "Epoch [39/50], Loss: 0.0445\n",
      "Epoch [40/50], Loss: 0.0533\n",
      "Epoch [41/50], Loss: 0.0579\n",
      "Epoch [42/50], Loss: 0.0395\n",
      "Epoch [43/50], Loss: 0.1093\n",
      "Epoch [44/50], Loss: 0.0235\n",
      "Epoch [45/50], Loss: 0.0248\n",
      "Epoch [46/50], Loss: 0.0457\n",
      "Epoch [47/50], Loss: 0.0287\n",
      "Epoch [48/50], Loss: 0.0887\n",
      "Epoch [49/50], Loss: 0.0661\n",
      "Epoch [50/50], Loss: 0.0207\n",
      "Training time: 562.80 seconds\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 50\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
    "end_time = time.time()\n",
    "print(f'Training time: {end_time - start_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a97697c-c4b7-4361-8733-472238f69ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test set: 99.03%\n",
      "Test time: 53.27 seconds\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    start_time = time.time()\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "    end_time = time.time()  # End timing\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f'Accuracy of the model on the test set: {100 * accuracy:.2f}%')\n",
    "print(f'Test time: {end_time - start_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3471bea-6e5c-4528-8c56-960f93e50e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      1.00      1.00      1909\n",
      "     Class 1       1.00      1.00      1.00      3540\n",
      "     Class 2       0.99      1.00      1.00      1877\n",
      "     Class 3       1.00      1.00      1.00      1324\n",
      "     Class 4       0.99      1.00      1.00      2544\n",
      "     Class 5       1.00      1.00      1.00      3761\n",
      "     Class 6       1.00      1.00      1.00      3400\n",
      "     Class 7       0.98      0.98      0.98     10707\n",
      "     Class 8       1.00      1.00      1.00      5893\n",
      "     Class 9       1.00      0.99      0.99      3114\n",
      "    Class 10       1.00      1.00      1.00      1015\n",
      "    Class 11       1.00      1.00      1.00      1831\n",
      "    Class 12       0.98      1.00      0.99       870\n",
      "    Class 13       0.99      0.98      0.99      1016\n",
      "    Class 14       0.98      0.96      0.97      6905\n",
      "    Class 15       1.00      1.00      1.00      1717\n",
      "\n",
      "    accuracy                           0.99     51423\n",
      "   macro avg       0.99      0.99      0.99     51423\n",
      "weighted avg       0.99      0.99      0.99     51423\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 1909     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [    0  3540     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [    0     0  1871     2     3     0     0     0     0     0     1     0\n",
      "      0     0     0     0]\n",
      " [    0     0     0  1322     2     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [    0     0     0     0  2542     0     0     0     0     2     0     0\n",
      "      0     0     0     0]\n",
      " [    0     0     0     0     1  3760     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [    0     0     0     1     0     1  3397     0     0     0     1     0\n",
      "      0     0     0     0]\n",
      " [    0     0     0     0     0     0     0 10538     0     0     0     0\n",
      "      0     0   169     0]\n",
      " [    0     0     0     0     0     0     0     0  5887     6     0     0\n",
      "      0     0     0     0]\n",
      " [    0     0    11     0    14     0     0     0     0  3082     0     0\n",
      "      0     7     0     0]\n",
      " [    0     0     1     0     0     0     0     0     0     0  1014     0\n",
      "      0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0  1831\n",
      "      0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     1\n",
      "    868     1     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     1     0     0\n",
      "     15   994     0     6]\n",
      " [    0     0     0     0     0     0     0   249     0     3     0     0\n",
      "      0     0  6653     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0  1717]]\n",
      "Kappa Accuracy: 0.9892\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "class_report = classification_report(all_labels, all_preds, target_names=[f'Class {i}' for i in range(16)])\n",
    "print('Classification Report:')\n",
    "print(class_report)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "# Kappa accuracy\n",
    "kappa = cohen_kappa_score(all_labels, all_preds)\n",
    "print(f'Kappa Accuracy: {kappa:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de7e8bfc-282b-4123-9a3e-9e0b8d07d5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy (OA): 0.9903\n",
      "Average Accuracy (AA): 0.9941\n",
      "Kappa Coefficient: 0.9892\n",
      "Class 1 Accuracy: 1.0000\n",
      "Class 2 Accuracy: 1.0000\n",
      "Class 3 Accuracy: 0.9968\n",
      "Class 4 Accuracy: 0.9985\n",
      "Class 5 Accuracy: 0.9992\n",
      "Class 6 Accuracy: 0.9997\n",
      "Class 7 Accuracy: 0.9991\n",
      "Class 8 Accuracy: 0.9842\n",
      "Class 9 Accuracy: 0.9990\n",
      "Class 10 Accuracy: 0.9897\n",
      "Class 11 Accuracy: 0.9990\n",
      "Class 12 Accuracy: 1.0000\n",
      "Class 13 Accuracy: 0.9977\n",
      "Class 14 Accuracy: 0.9783\n",
      "Class 15 Accuracy: 0.9635\n",
      "Class 16 Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Overall Accuracy\n",
    "oa = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "# Calculate per-class accuracy from the confusion matrix\n",
    "class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "# Average Accuracy\n",
    "aa = np.mean(class_accuracy)\n",
    "\n",
    "# Kappa Coefficient\n",
    "kappa = cohen_kappa_score(all_labels, all_preds)\n",
    "\n",
    "print(f'Overall Accuracy (OA): {oa:.4f}')\n",
    "print(f'Average Accuracy (AA): {aa:.4f}')\n",
    "print(f'Kappa Coefficient: {kappa:.4f}')\n",
    "for i, acc in enumerate(class_accuracy): print(f'Class {i+1} Accuracy: {acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
